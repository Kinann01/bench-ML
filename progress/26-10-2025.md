### JVM and java bytecode? Tiered compilation?

1) Initially we have the build stage where javac compiles everything into platform-independent java bytecode and puts everything in a .jar based on JVM specification

2) The JVM is started up. We have the class loading engine, linking which includes, java bytecode verification, preparation and resolution of symbolic symbols and lastly run clinit

3) Execution & tiered compilation. The JVM starts in the interpreter (Tier 0), which both executes the bytecode and collects profiles (invocation counts, loop back-edge counts, branch frequencies, etc). Hotness comes from two sources: (a) method invocations and (b) hot loops; hot loops can trigger On-Stack Replacement (OSR) so execution can jump into compiled code at a loop header. As more profile data is collected, methods are compiled by C1 (Tiers 1–3) for fast startup and low latency; these tiers insert profiling hooks and provide average/modest optimizations. Very hot code is promoted to Tier 4 (C2), which uses the collected profiles to generate very highly optimized native code but in this case compiling usually takes more time. Many optimizations are speculative. Compiled code contains guards and uncommon traps; if an assumption later becomes false (e.g. a new subclass loads that overrides a method), the JVM deoptimizes which basically means that control is transferred to the interpreter (or a lower tier), the compiled method becomes a zombie method, and the code may be recompiled with updated profiles.

-> Can be interesting regarding zombie methods: https://stackoverflow.com/questions/2930838/java-printcompilation-output-whats-the-meaning-of-made-not-entrant-and-made#7138079

4) Part of the JVM is also the run time environment contains memory management, GC, thread management, java native interface, native method libraries and so on.

### Recap

- Each default.csv file corresponds to a single JVM process of a single workload with a single setting/metadata
- Each instance of a dataset is a single cycle of execution of the corresponding benchmark
- The harness a total execution duration of ~5–10 minutes and workload inputs are chosen so each repetition is roughly 1–10s. Hence sometimes we see less/more instances within a default.csv

### From the understanding of JVM and the whole process of Java compilation, what can we infer about the data we have?

- Early repetitions: We expect higher **JVM_COMPILATIONS** and large **compilation_time_ms**. **compilation_total_ms** is non-decreasing - will only go up if JIT compilations take place. **iteration_time_ns** should drop over the first few reps.
- Middle repitions: Compile spikes still can exist; promotions in the compilations and we should see the value **iteration_time_ns** slowly converging to some value
- Late repetitions: **JVM_COMPILATIONS** ~ 0 or very low; **compilation_time_ms** near 0 as well. **iteration_time_ns** flat aside from GC noise.
- We can see instances of deoptimization, as explained above, where we can see mid run spikes in **JVM_COMPILATIONS** and **compilation_time_ms** along with **iteration_time_ns** and then we recover as recompiled code runs.
- We assume the above is how it should look like...

### Based on what we learned and find above, we should find a subset of each default.csv that we are interested in. Which and how do we decide?

- We want to focus on the steady part of each execution, this is where we assume warmup has already occurred and compilation_time_ms should be near 0 because at that point JIT has stopped compiling new code and the **iteration_time_ns** is converging to its plateau. There is a reason to why the total execution was ~5-10 minutes with each repition being between 1-10s and this is to maximize variation in order for us to have  to find the right subset/window that we are interested in. It is still unclear what is the best way to define that "steady part" of the data. This step will correspond to the filtering part that takes place prior to merging, we pick the interestin and clean part of the data that should define the metrics as precise as possible in that particular commit. # TODO: How to find this sweet spot and provide a proof of concept. We want to have a strong basis on why a certain subset/suffix of the dataset was picked in the filtering phase.

### Assume commits A and B, how can you define a regression that occurred due to commit B? How are you defining this "regression"?

- Performance engineers should be primariy interested in **interation_time_ns**. We can consider an increase in **iteration_time_ns** and **ref_cycles** as two examples of regression introduced by a commit B. JIT is doing a bad job here if **iteration_time_ns** is slower
- Another example is compiler overhead regression. For instance **iteration_time_ns** remains steady/unchanged but we see an increase in **JVM_COMPILATIONS** or **compilation_time_ms**.

### recap - features

- **iteration_time_ns**, **ref_cycles**, **JVM_COMPILATIONS**, **compilation_time_ms** should be the most interesting. If other metrics can help with filtering, we will add other things.
- GC data we have, can also be used to learn more about noise that can be introduced to **iteration_time_ns** at instances. # TODO

### recap/elaborate on pipeline.

1) Read configuration provided by user
2) Find relevant datasets based on configuration
3) filter
4) merge and construct a timeline of data
5) Data mining and ML techniques
6) Report + UI + graphs + analysis

### Next

- I think we have to focus on step number (3) of the pipeline. Once we are done with that, we can successfully merge the datasets to construct a timeseries and then start looking into how to represent it as a ML problem.
- Check this paper, can it help? https://research.spec.org/icpe_proceedings/2015/proceedings/p15.pdf
- Look for other relevant papers where ML techniques were utilized in detecting performance improvemenet/regression over a timeline of commits.
