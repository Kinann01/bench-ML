### Config? Parser? traverse and find the relevant datasets?

- See /src/config.json /src/main.py and /src/config.py
- See /src/finder.py

### Implement generic logger that will be used throughout the pipeline to log (info, warn, error)

- See /src/logger.py

### Set up the main dataset pipeline for filtering and merging the datasets

- See /src/pipeline.py 

### Apply the "filter" on each found default.csv to detect steady state prior to merging them.

- See /src/detectors.py

- The paper quotes "In our own evaluation, waiting for a window of reduced compilation activity, as indicated by the reported compiler thread execution duration, provided reasonably reliable results." Considering the dataset we have. i.e. we have information regarding compiler activity and the actual performance. I have implement an approach that utilizes the compiler activity data and the actual performance (iteration_time_ns) to detect the steady state as precise as possible. 

- I believe Steady State Detection is an important part of our thesis because when it comes to analysing the data. We want to make sure the data is as clean as possible.

- I have represented the problem as a signal analysis problem for Compiler Activity and Performance Outcome such that we have a cause and an effect. So the main question is when does the cause finish (the compiler activity) and when does the effect take place (the performance outcome)?

- Compiler Activity Signal (CAS) analysis will show us the first main dip from when compilation activity is ending and Performance Outcome Signal (POS) will show us the start of the effect. For that I have implemented a hybrid approach using the warm up detection mentioned in the paper and utilized a new approach used for determining steady state in performance called the KB-KSSD (Kernel Based - Kelly's Steady State Detection) from [1]. I have used a few tricks from there. In particular, the smoothening approach to get rid of high frequency noise and maintaing the trend, and a discrete kernel convolution to locate the phase shift. The kernel is built based on the shape we are looking for. The paper uses a kernel window of the form `[...,1,1,-1,-1, ...]` where the positive half detects the high iteration times of the warmup phase, and the negative half detects the low iteration times. After the convolution step, we will have the result which contains the values of the convolution at each index t. The maximum of these results will be the dip as it will reflect the drop to the low and steady iteration times.

- Unfortunately, I have not gotten to test the detect_cutoff_index() but the pipeline is set. I am missing determining the parameters (i.e. window sizes, thresholds, ...). I think the paramters will be automatically set based on the size of the dataset. I also have other below TODOs.

```bash
➜  bench-ML git:(main) ✗ grep -r "TODO" src
src/finder.py:        return self._paths # TODO: return a RO proxy - we dont want to call .copy() to avoid new allocation
src/finder.py:        return self._version_cache # TODO: return a RO proxy - we dont want to call .copy() to avoid new allocation
src/plotter.py:    if cas_cutoff_index is None and pos_cutoff_index is None: # TODO: Check this before calling _save_timeseries_plot()
src/plotter.py:            if cutoff_index is not None: # TODO: out of range indices and negative indices should be maybe covered?
src/plotter.py:                n = len(series) # TODO: chunk by chunk is better?
src/pipeline.py:        self.versions = versions # TODO: We will probablly need this to add context to each dataset
src/detectors.py: Major TODOs in detectors.py:
src/detectors.py:        n = len(series) # TODO: Chunk by chunk is better?
src/main.py:        plotter = Plotter(ROOT / "plots") # TODO: Should it be fixed or should the user decide?
src/main.py:        """TODO: error handling + pass the merged datasets to the analyzer """
```

- Moreover, I stumbled into [1] and I decided to try out the kb-kssd. Its a new and interesting approach and I felt like its worth spending some time on it. I am also writing a Medium article about this. If it is deemed that this approach is useless/unnecessary. The compiler activity analysis is there and we can just fallback to it, very little code change will be necessary. The code saves all the plots to a plots directory for manual inspection. It plots both the cutoff indices of the CAS analysis and the POS analysis so we will be able to run some tests on different configs and see the results of the SSD.

### Implement plotter and save pngs to directory.
- see /src/plotter.py

[1] https://arxiv.org/html/2506.04204v1
