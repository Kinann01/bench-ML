### What is the data model? How is the data organized?

**Repository** (name)
  - Source repo being tested (e.g. ftp-graal-ce-master-jdk-17)

**Version** (tag, hash, time, Repository)
  - Note that tag can be empty. e.g. "87557": {
                                      "tag": "",
                                      "time": "2022-07-02T07:37:58+00:00",
                                      "hash": "<HASH>",
                                      "repository": 13
                                    }"

**BenchmarkWorkload** (name)
  - A specific workload that points to a BenchmarkType and contains the BenchmarkWorkload name.

**BenchmarkType** (name, label, description)
  - The category to which the BenchmarkWorkload belongs (Renaissance, DaCapo, ScalaBench, SPECjvm2008 variant).

**MachineHost** (name) && **MachineType** (name, label, description)
  - Machine host points to a machine type which is a physical host with a name/label/specs

**PlatformType** (name, label, description)
  - PlatformType contains project related info like distro and JDK.
  - Important keywords to consider here is:
    * EE or CE - Enterprise edition or Community edition.
    * Release or devel/master
    * JDK 8, 11, 17 - Jave baseline the distro is built on
    * graal/native

**PlatformInstallation**
  - Points to **PlatformType** and the **Version**.

**Configuration** (name, label, description)
  - GC Build configurations
  - vanilla/native where vanilla runs on the JVM with a JIT compiler while native runs as a GraalVM native image (binaries compiled AOT)
  - PGO (Profile Guided optimizations)
  - 12GS or 12GX
  - GC specific flags
  - GC Flavour
  - etc.
  - See (*) below in the 'Notes' section

**Measurement** and **ResultFile**
  - Triple Nested directories containing ResultFile
  - -- 81
      -- 27
          -- 28
              -- 9282781
                  |-- config-hash.txt
                  |-- default.csv
                  |-- metadata
                  |-- platform-command.txt
                  |-- platform-stderr.txt
                  ...

### Notes:

- The metadata of a dataset contains important information that will allow us to separate and isolate datasets that are of interest. These datasets can be merged and they will be representing the data over a timeline. This is the main thing as this will allow us to understand how the performance changed over time where time in our case is a 'commit'

- Example of a metadata {
                          "create_time": "2022-07-02T15:31:31.842025+00:00",
                          "status_time": "2022-07-02T15:40:39.963746+00:00",
                          "machine_host": 15,
                          "platform_installation": 40922,
                          "benchmark_workload": 108,
                          "configuration": 43
                        }

- **machine_host** will allow us to separate between platform type 5 or platform type 6. We will focus on one of them
- **plaftorm_installation** is more diverse. With platform_installation, we will need to focus on one platform_type. Corresponding to the platform_type we have a version, the version contains the repo, and the time. So pick a specific platform type, for instance, 15 (graal-ee-release-jdk-8), and we will look at all the datasets associated with that same platform_type and version. They should have different "time" parameter and those time parameters will help construct a timeline.
- **BenchmarkWorkload** is straightforward. This will help us separate BenchmarkType because it doesn't make sense to merge data of different BenchmarkType
- **configuration** is also straightfoward. It is good to get familiar with the details because understanding those details can help learn/adapt more information regarding the GC-related data we have collected but so far we focus on vanilla GC (normal JVM with JIT) and we assume that is sufficient (*)

- Each default.csv file corresponds to a single JVM process of a single workload with a single setting/metadata
- Each instance of a dataset is a single cycle of execution of the corresponding benchmark
- The harness a total execution duration of ~5–10 minutes and workload inputs are chosen so each repetition is roughly 1–10s. Hence sometimes we see less/more instances within a default.csv

- Questions:
Look at the project build info and at repository. Is there any correspondance that we should be aware of?
How important is it to understand ./configuration/metadata? Is it going to help us when to comes to working with GC specific metrics?

### What are the features?
  * total_ms - Aggregate - The duration from the JVM start to the end of the current repetition.
  * iteration_time_ns - per rep - main performance per rep. The duration of the current repetition measured using wall clock time.
  * process_cpu_time_ns - per rep - CPU vs wall time. The aggregate thread execution duration of the current repetition.
  * ref_cycles - per rep - hardware specific metrics collected per rep.
  * JVM_COMPILATIONS - aggregate - Number of JIT events reported by the JVM
  * compilation_time_ms - aggregate - The aggregate execution duration of the compiler threads in this repetition.
  * compilation_total_ms - aggregate - The aggregate execution duration of the compiler threads from the JVM start.
  * jmx_memory_used_size - per rep - The size of the used heap at the end of the current repetition.
  * jmx_memory_used_delta
  * jmx_memory_old_collection_count
  * jmx_memory_old_collection_total_ms
  * jmx_memory_young_collection_count
  * jmx_memory_young_collection_total_ms

### Do we have to pick a subset of the features?

- I think yes. We will have to pick a subset of the features but this is easier to answer later once we know what exactly do we need to feed our ML model.

### Do we need to perform any filtering?

- Yes because we need to consider the fact that a single default.csv contains a whole run of compiling and running the application. Start-up, loading the classes, some warming up, compilation, execution and GC events. These will speak to the behavior of the metrics collected

### How will the pipeline look like?

- The way I see it so far.

* Input the datasets -> filter out datasets of interest based on some config -> filter out the data within the datasets of interest -> merge the datasets
  -> construct a timeline -> ML model + data mining to deterect performance changes -> explanations/visualization -> report

* Until we are able to fully understand the datasets, our ability to construct a timeline and how we are going to construct it, we won't be able to determine what ML techniques we will be using but I highly see this mainly as using unsupervised learning along with data damining using statistical analysis of domain specific knowledge.

### - What are we considering as a regression or an improvement?

* Not sure yet. Need to study the data more.
